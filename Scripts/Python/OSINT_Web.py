import os
#import platform
import requests
import re
import socket
from bs4 import BeautifulSoup

#Colors
BLUE = "\033[94m"
GREEN = "\033[92m"
RESET = "\033[0m"


def logo():
    ascii_logo = f"""{BLUE}
⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⣠⣤⣶⣶⣾⣿⣿⣿⣿⣷⣶⣄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣠⣶⣾⣿⣿⣿⣿⣷⣶⣶⣤⣄⣀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀
⠀⠀⠀⠀⠀⢀⣠⡴⠾⠟⠋⠉⠉⠀⠀⠀⠀⠀⠀⠀⠈⠉⠉⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠉⠉⠁⠀⠀⠀⠀⠀⠀⠀⠉⠉⠙⠛⠷⢦⣄⡀⠀⠀⠀⠀⠀
⠀⠀⠀⠀⠘⠋⠁⠀⠀⢀⣀⣤⣶⣖⣒⣒⡲⠶⣤⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣤⠶⢖⣒⣒⣲⣶⣤⣀⡀⠀⠀⠈⠙⠂⠀⠀⠀⠀
⠀⠀⠀⠀⠀⠀⠀⣠⢖⣫⣷⣿⣿⣿⣿⣿⣿⣶⣤⡙⢦⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⡴⢋⣤⣾⣿⣿⣿⣿⣿⣿⣾⣝⡲⣄⠀⠀⠀⠀⠀⠀⠀
⠀⠀⠀⣄⣀⣠⢿⣿⣿⣿⣿⣿⣿⣿⡿⠟⠻⢿⣿⣿⣦⣳⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣟⣴⣿⣿⡿⠟⠻⢿⣿⣿⣿⣿⣿⣿⣿⡻⣄⣀⣤⠀⠀⠀
⠀⠀⠀⠈⠟⣿⣿⣿⡿⢻⣿⣿⣿⠃⠀⠀⠀⠀⠙⣿⣿⣿⠓⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠚⣿⣿⣿⠋⠀⠀⠀⠀⠘⣿⣿⣿⡟⢿⣿⣿⣟⠻⠁⠀⠀⠀
⠤⣤⣶⣶⣿⣿⣿⡟⠀⣿⣿⣿⡇⠀⠀⠀⠀⠀⠀⢻⣿⣿⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⣿⣿⡏⠀⠀⠀⠀⠀⠀⣹⣿⣿⣷⠈⢻⣿⣿⣿⣶⣦⣤⠤
⠀⠀⠀⠀⠀⢻⣟⠀⠀⣿⣿⣿⣿⡀⠀⠀⠀⠀⢀⣿⣿⡟⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⢻⣿⣿⡀⠀⠀⠀⠀⢀⣿⣿⣿⣿⠀⠀⣿⡟⠀⠀⠀⠀⠀
⠀⠀⠀⠀⠀⠀⠻⣆⠀⢹⣿⠟⢿⣿⣦⣤⣤⣴⣿⣿⣿⠃⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠘⣿⡿⢷⣤⣤⣤⣴⣿⣿⣿⣿⡇⠀⣰⠟⠀⠀⠀⠀⠀⠀
⠀⠀⠀⠀⠀⠀⠀⠙⠂⠀⠙⢀⣀⣿⣿⣿⣿⣿⣿⣿⠟⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠻⠁⠀⣻⣿⣿⣿⣿⣿⣿⠏⠀⠘⠃⠀⠀⠀⠀⠀⠀⠀
⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⡈⠻⠿⣿⣿⣿⡿⠟⠋⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠘⠻⢿⣿⣿⣿⠿⠛⢁⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀
⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠚⠛⣶⣦⣤⣤⣤⡤⠆⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠰⢤⣤⣤⣤⣶⣾⠛⠓⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀
{RESET}{GREEN}
\t▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄{GREEN}
\t█{GREEN}[+] Author: firodrigz{RESET}                    {GREEN}█{GREEN}
\t█{GREEN}[+] Follow me on Github: firodrigz{RESET}       {GREEN}█{GREEN}
\t█{GREEN}[+] Version: 1.0{RESET}                         {GREEN}█{GREEN}
\t█{GREEN}[+] Date: 25-01-2024{RESET}                     {GREEN}█{GREEN}
\t▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀▀
"""
    return print(ascii_logo)

def subdominios(lista, destino):
    total_subdominios = len(lista)
    existente = []

    for elemento in lista:
        subdominio = f"{elemento}.{destino}"  
            
        url = f"http://{subdominio}"
        try:
            respuesta = requests.get(url, timeout=2)
            
            if respuesta.status_code == 200:
                print(f"El subdominio {subdominio} está alcanzable (código de respuesta 200).")
                existente.append(subdominio)


        except requests.RequestException as e:
            pass

    return existente

    #for elemento in lista:
    #    subdominio = f"{elemento}.{destino}"  
            
    #    comando_ping = f"ping -c 4 {subdominio} > /dev/null 2>&1"
    #    resultado = os.system(comando_ping)

    #    if resultado == 0:
    #        print(f"El subdominio {subdominio} existe.")


def encuentra_ip(subdominios_existentes):
    for elemento in subdominios_existentes:
        direccion_ip = socket.gethostbyname(elemento)
        print(f"La dirección IP del subdominio {elemento} es: {direccion_ip}")


def identificar_tecnologias(dominio):
    url = f'https://api.wappalyzer.com/lookup/v1/?urls[]={dominio}'

    try:
        response = requests.get(url)
        response.raise_for_status()
    except requests.exceptions.RequestException as e:
        print(f"Error al realizar la solicitud a Wappalyzer: {e}")
        return []

    try:
        datos_wappalyzer = response.json()
    except ValueError as e:
        print(f"Error al decodificar la respuesta JSON de Wappalyzer: {e}")
        return []

    tecnologias_identificadas = []
    for sitio in datos_wappalyzer.get('urls', []):
        for aplicacion in sitio.get('technologies', []):
            nombre = aplicacion.get('name')
            version = aplicacion.get('version')
            tecnologia = f"{nombre} {version}" if version else nombre
            tecnologias_identificadas.append(tecnologia)

    return tecnologias_identificadas


def scraping(dominio):
    try:
        response = requests.get(dominio)
        imprimir=response.raise_for_status() # Lanza una excepción para códigos de error HTTP
    except requests.exceptions.RequestException as e:
        print(f"Error al realizar la solicitud HTTP: {e}")
        return

    try:
        soup = BeautifulSoup(response.text, 'html.parser')
    except Exception as e:
        print(f"Error al analizar el HTML: {e}")
        return

    
    # Configuración para extraer diferentes tipos de datos
    datos_a_extraer = [
        {'tipo': 'enlaces', 'etiqueta': 'a', 'atributo': 'href'},
        {'tipo': 'titulos', 'etiqueta': 'h2'},
        {'tipo': 'correos', 'patron': r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b'},
        {'tipo': 'telefonos', 'patron': r'\b(?:\+\d{1,2}\s?)?(?:\(\d{1,4}\)|\d{1,4})[-.\s]?\d{1,12}\b'},
        {'tipo': 'direcciones', 'etiqueta': 'address'},
        {'tipo': 'posible versión WordPress', 'etiqueta': 'meta', 'atributo': 'content', 'atributo_name': 'generator'},
        {'tipo': 'imágenes', 'etiqueta': 'img', 'atributo': 'src'},
        {'tipo': 'última fecha modificación', 'etiqueta': 'meta', 'atributo': 'content', 'atributo_name': 'last-modified'},
        {'tipo': 'tecnologias', 'etiqueta': None, 'atributo': None, 'patron': None}
    ]

    resultados = {}

    for config in datos_a_extraer:
        tipo = config.get('tipo')
        etiqueta = config.get('etiqueta')
        atributo = config.get('atributo', None)
        atributo_name = config.get('atributo_name', None)
        patron = config.get('patron', None)

        if tipo == 'tecnologias':
            tecnologias = identificar_tecnologias(dominio)
            resultados[tipo] = tecnologias

        if etiqueta:
            if atributo_name == 'generator':
                elementos = soup.find_all(etiqueta, {'name': atributo_name})
            elif atributo_name == 'last-modified':  
                elementos = soup.find_all(etiqueta, {'http-equiv': atributo_name})
            else:
                elementos = soup.find_all(etiqueta)

            if atributo:
                datos = [elemento.get(atributo) for elemento in elementos]
            else:
                datos = [elemento.text.strip() for elemento in elementos]

        elif patron:
            # Búsqueda usando expresiones regulares
            texto_completo = soup.get_text()
            datos = re.findall(patron, texto_completo)
        
        resultados[tipo] = datos


        # Filtrar números de teléfono con 4 dígitos o menos
        if 'telefonos' in resultados:
            resultados['telefonos'] = [telefono for telefono in resultados['telefonos'] if len(re.sub(r'\D', '', telefono)) > 4]


    # Imprimir resultados dentro de la función
    for tipo, datos in resultados.items():
        print(f"{tipo}: {datos}")
    
    # Extraer todos los enlaces de la página
    #links = soup.find_all('a')
    #for link in links:
    #    print(link.get('href'))

def menu():

    while True:
        print(f"{BLUE}Help menu..   press 1 to continue"+ RESET)
        print(f"{BLUE}[+] 1.{RESET} SUBDOMAIN FUZZING")
        print(f"{BLUE}[+] 2.{RESET} SCRAPING")
        print(f"{BLUE}[+] 0.{RESET} EXIT")
        try:
            value = input("> ")
            
            if value == "1":
                mi_lista = ['www', 'biondi3', 'ns1', 'ns2', 'ns3', 'blog', 'mail', 'app']
                dominio_principal = input("[+] Ingrese dominio: ")
                subdominios_existentes = subdominios(mi_lista, dominio_principal)
                if len(mi_lista) > 0:
                    confirm = input("\n[+] Desea continuar y encontrar las IP? Y/n (presione Enter para seleccionar Y): ") or "Y"
                    if confirm.lower() == "y" or confirm.lower() == "yes":
                        encuentra_ip(subdominios_existentes)
                            
            elif value == "2":
                dominio = input("[+] Ingrese dominio: ")
                scraping(dominio)

            elif value == "0":
                break
        
        except KeyboardInterrupt:
            print("\nTerminando el programa...")
            break

if __name__ == "__main__":
    logo()
    menu()